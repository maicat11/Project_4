{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "import os\n",
    "import regex as re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct = set(string.punctuation.replace('\\\\','').replace('|','').replace(\"'\",''))\n",
    "\n",
    "pos_punct_info = open(\"data/processed/output_POS.txt\", 'a')\n",
    "\n",
    "#check avg sent size\n",
    "pos_punct_info.write(\"book_name|total_words|avg_sentence_size|\"\n",
    "                     + \"!|#|\\\"|%|$|&|(|)|+|*|-|,|/|.|;|:|=|<|?|>|\"\n",
    "                     + \"@|[|]|_|^|`|{|}|~|neg|neu|pos|compound|\"\n",
    "                     + \"Title|Author|CC|CD|DT|EX|FW|IN|JJ|JJR|JJS|\"\n",
    "                     + \"LS|MD|NN|NNP|NNPS|NNS|PDT|PRP|PRP$|RB|RBR|\" \n",
    "                     + \"RBS|RP|VB|VBD|VBG|VBP|VBN|WDT|VBZ|WRB|WP$|WP|\")\n",
    "pos_punct_info.write('\\n') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_and_words(character_list):\n",
    "    \"\"\"\n",
    "    Iterate through all characters. Count periods, punctuation frequencies. \n",
    "    word_count = words in sentence (resets to zero after a period). \n",
    "    total_words is the book's total word count.\n",
    "    \"\"\"\n",
    "    punctuation_dict = defaultdict(int)\n",
    "    sentence_count = 0\n",
    "    word_count = 0\n",
    "    period_count = 0\n",
    "    avg_sent_size = 0\n",
    "    total_words = 0\n",
    "    punct_count = 0\n",
    "    \n",
    "    #sentence count\n",
    "    for i in range(1, len(character_list)):\n",
    "        #if letter followed by space or punct, then word count +=1\n",
    "        if ((character_list[i] == \" \" or str(character_list[i]) in punct) and \n",
    "            str(character_list[i-1]) in string.ascii_letters):\n",
    "            total_words += 1\n",
    "        #count periods \n",
    "        if character_list[i] == \".\":\n",
    "            period_count += 1\n",
    "        if character_list[i] in punct:\n",
    "            punct_count += 1\n",
    "            punctuation_dict[character_list[i]] += 1\n",
    "\n",
    "            \n",
    "    avg_sent_size = (total_words/period_count)\n",
    "    #put together output, bar delimited\n",
    "    pos_punct_info.write(str(total_words) + \"|\")\n",
    "    pos_punct_info.write(str(avg_sent_size) + \"|\")\n",
    "    \n",
    "    for p in punct:\n",
    "        s = \"\"\n",
    "        if p in punctuation_dict:\n",
    "            s = s + str(punctuation_dict[p] / punct_count) + \"|\"    #ratio of punct that is [x]\n",
    "        else:\n",
    "            s = s + str(0) + \"|\"                                     #0 if unused\n",
    "        pos_punct_info.write(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment(temp):\n",
    "    temp = temp.replace('\\n', '')\n",
    "    temp = temp.replace('\\r', '')\n",
    "    # tokenize sentences \n",
    "    content = tokenize.sent_tokenize(temp)\n",
    "    \n",
    "    #get author and title now that content is split by sentence \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    booksent = []\n",
    "    for sentence in content:\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        ssarray = [ss['neg'], ss['neu'], ss['pos'], ss['compound']]\n",
    "        booksent.append(ssarray)\n",
    "    valuearray = np.array(booksent)\n",
    "    # mean negative, neutral, positive, compound score for all lines in book\n",
    "    values = np.mean(valuearray, axis=0)\n",
    "    return values, booksent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book = 'James_DevicesAndDesires.txt'\n",
    "# with open(\"data/interim/\" + book, 'r') as f:\n",
    "#     content = f.read().rstrip('\\n')\n",
    "# get_sentiment(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_author(book_title):\n",
    "    book_list = {'Agatha Christie': ['AndThenThereWereNone', \n",
    "                                     'DestinationUnknown', \n",
    "                                     'ElephantsCanRemember'], \n",
    "                    'Iris Murdoch': ['TheSandcastle', \n",
    "                                     'TheBlackPrince', \n",
    "                                     'JacksonsDilemma'], \n",
    "                      'P.D. James': ['DevicesAndDesires', \n",
    "                                     'DeathComesToPemberley', \n",
    "                                     'CoverHerFace']\n",
    "                }\n",
    "    \n",
    "    for author, books in book_list.items():\n",
    "        if book_title in books:\n",
    "            return author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_tagging(content):\n",
    "    parts = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \n",
    "             \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNP\", \"NNPS\", \n",
    "             \"NNS\", \"PDT\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \n",
    "             \"RBS\", \"RP\", \"VB\", \"VBD\", \"VBG\",  \"VBP\", \n",
    "             \"VBN\", \"WDT\", \"VBZ\", \"WRB\", \"WP$\", \"WP\" ]\n",
    "    # tokenize first\n",
    "    text = nltk.word_tokenize(content)  \n",
    "    results = nltk.pos_tag(text)\n",
    "    \n",
    "    #dict of {POS: count}\n",
    "    results_dict = defaultdict(int)\n",
    "    counter = 0\n",
    "    for tag in results:\n",
    "        token = tag[0]\n",
    "        pos = tag[1]\n",
    "        counter += 1\n",
    "        results_dict[pos] += 1\n",
    "\n",
    "    #write to file\n",
    "    for part_of_sp in parts:\n",
    "        s = \"\"\n",
    "        if part_of_sp in results_dict:\n",
    "            #percent of POS \n",
    "            s = s + str(results_dict[part_of_sp]/float(counter)) + \"|\"    \n",
    "        else:\n",
    "            s = s + str(0) + \"|\"  #0 if unused                               \n",
    "        pos_punct_info.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    '''\n",
    "    read file as a list of words\n",
    "    set lowercase, stem, remove stopwords???\n",
    "    get punctuation string for later feature extraction\n",
    "    save local wordcount dict???\n",
    "    save global word dict after finished looping through docs???\n",
    "    '''\n",
    "    for book in os.listdir(\"data/interim\"):\n",
    "        book_file = str(book)\n",
    "        book_name = re.sub(r'(James_|Murdoch_|Christie_|\\.txt)*', '', book_file)\n",
    "        title = re.sub(\"([a-z])([A-Z])\",\"\\g<1> \\g<2>\", book_name)\n",
    "        pos_punct_info.write(book_name + \"|\")\n",
    "        \n",
    "        with open(\"data/interim/\" + book_file, 'r') as f:\n",
    "            content = f.read().rstrip('\\n')\n",
    "            \n",
    "        punct_and_words(content)\n",
    "        sentiment_values, _ = get_sentiment(content)\n",
    "        neg = sentiment_values[0]\n",
    "        neu = sentiment_values[1]\n",
    "        pos = sentiment_values[2]\n",
    "        compound = sentiment_values[3]\n",
    "        pos_punct_info.write(str(neg) + \"|\" \n",
    "                             + str(neu) + \"|\" \n",
    "                             + str(pos) + \"|\" \n",
    "                             + str(compound) + \"|\")\n",
    "        \n",
    "        title = re.sub(\"([a-z])([A-Z])\",\"\\g<1> \\g<2>\", book_name)\n",
    "        author = get_author(book_name)\n",
    "        pos_punct_info.write(title + \"|\" + author + \"|\")\n",
    "        pos_tagging(content)\n",
    "        pos_punct_info.write('\\n')\n",
    "        print(f'Done processing: {title}')                                                 \n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing Devices And Desires\n",
      "Done processing The Sandcastle\n",
      "Done processing Destination Unknown\n",
      "Done processing Elephants Can Remember\n",
      "Done processing Death Comes To Pemberley\n",
      "Done processing Cover Her Face\n",
      "Done processing Jacksons Dilemma\n",
      "Done processing And Then There Were None\n",
      "Done processing The Black Prince\n"
     ]
    }
   ],
   "source": [
    "# book_name = 'Test Book'\n",
    "# pos_punct_info.write(book_name + \"|\")\n",
    "preprocessing()\n",
    "pos_punct_info.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 71)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>total_words</th>\n",
       "      <th>avg_sentence_size</th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>\"</th>\n",
       "      <th>%</th>\n",
       "      <th>$</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>(</th>\n",
       "      <th>...</th>\n",
       "      <th>VBD</th>\n",
       "      <th>VBG</th>\n",
       "      <th>VBP</th>\n",
       "      <th>VBN</th>\n",
       "      <th>WDT</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>WRB</th>\n",
       "      <th>WP$</th>\n",
       "      <th>WP</th>\n",
       "      <th>Unnamed: 70</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DevicesAndDesires</td>\n",
       "      <td>157206</td>\n",
       "      <td>14.981988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067373</td>\n",
       "      <td>0.015981</td>\n",
       "      <td>0.022169</td>\n",
       "      <td>0.022631</td>\n",
       "      <td>0.003464</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheSandcastle</td>\n",
       "      <td>113888</td>\n",
       "      <td>13.673670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083745</td>\n",
       "      <td>0.018261</td>\n",
       "      <td>0.014774</td>\n",
       "      <td>0.020327</td>\n",
       "      <td>0.005340</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DestinationUnknown</td>\n",
       "      <td>60461</td>\n",
       "      <td>10.251102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062762</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>0.030155</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.016213</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.006028</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElephantsCanRemember</td>\n",
       "      <td>60024</td>\n",
       "      <td>11.163102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064873</td>\n",
       "      <td>0.010579</td>\n",
       "      <td>0.040799</td>\n",
       "      <td>0.017196</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.015221</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeathComesToPemberley</td>\n",
       "      <td>90969</td>\n",
       "      <td>19.236414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067453</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>0.012460</td>\n",
       "      <td>0.032502</td>\n",
       "      <td>0.005054</td>\n",
       "      <td>0.010069</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CoverHerFace</td>\n",
       "      <td>77527</td>\n",
       "      <td>13.320790</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073576</td>\n",
       "      <td>0.014477</td>\n",
       "      <td>0.015781</td>\n",
       "      <td>0.026823</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>0.010517</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.005231</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JacksonsDilemma</td>\n",
       "      <td>88190</td>\n",
       "      <td>17.110982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076244</td>\n",
       "      <td>0.021844</td>\n",
       "      <td>0.019245</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>0.008986</td>\n",
       "      <td>0.006288</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.005434</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AndThenThereWereNone</td>\n",
       "      <td>52467</td>\n",
       "      <td>8.815020</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073138</td>\n",
       "      <td>0.013511</td>\n",
       "      <td>0.017264</td>\n",
       "      <td>0.018840</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.005569</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TheBlackPrince</td>\n",
       "      <td>132907</td>\n",
       "      <td>12.994427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057163</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>0.032650</td>\n",
       "      <td>0.016863</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.005086</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã— 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               book_name  total_words  avg_sentence_size         !        #  \\\n",
       "0      DevicesAndDesires       157206          14.981988  0.000000  0.00000   \n",
       "1          TheSandcastle       113888          13.673670  0.000000  0.00000   \n",
       "2     DestinationUnknown        60461          10.251102  0.000000  0.00000   \n",
       "3   ElephantsCanRemember        60024          11.163102  0.000000  0.00000   \n",
       "4  DeathComesToPemberley        90969          19.236414  0.000000  0.00000   \n",
       "5           CoverHerFace        77527          13.320790  0.000875  0.00000   \n",
       "6        JacksonsDilemma        88190          17.110982  0.000000  0.00000   \n",
       "7   AndThenThereWereNone        52467           8.815020  0.000209  0.00007   \n",
       "8         TheBlackPrince       132907          12.994427  0.000000  0.00000   \n",
       "\n",
       "   \"         %  $         &         (     ...            VBD       VBG  \\\n",
       "0  0  0.000000  0  0.000000  0.000000     ...       0.067373  0.015981   \n",
       "1  0  0.000000  0  0.000000  0.000000     ...       0.083745  0.018261   \n",
       "2  0  0.000000  0  0.000000  0.000000     ...       0.062762  0.012496   \n",
       "3  0  0.000000  0  0.000000  0.000000     ...       0.064873  0.010579   \n",
       "4  0  0.000000  0  0.000000  0.000000     ...       0.067453  0.015016   \n",
       "5  0  0.000088  0  0.000000  0.000000     ...       0.073576  0.014477   \n",
       "6  0  0.000000  0  0.000000  0.000000     ...       0.076244  0.021844   \n",
       "7  0  0.000000  0  0.000000  0.001113     ...       0.073138  0.013511   \n",
       "8  0  0.000000  0  0.000048  0.000000     ...       0.057163  0.017470   \n",
       "\n",
       "        VBP       VBN       WDT       VBZ       WRB       WP$        WP  \\\n",
       "0  0.022169  0.022631  0.003464  0.012071  0.005285  0.000094  0.004886   \n",
       "1  0.014774  0.020327  0.005340  0.007362  0.003926  0.000117  0.004087   \n",
       "2  0.030155  0.017879  0.002633  0.016213  0.004376  0.000039  0.006028   \n",
       "3  0.040799  0.017196  0.004067  0.015221  0.005008  0.000052  0.009167   \n",
       "4  0.012460  0.032502  0.005054  0.010069  0.005618  0.000185  0.004510   \n",
       "5  0.015781  0.026823  0.003134  0.010517  0.004963  0.000145  0.005231   \n",
       "6  0.019245  0.021162  0.002862  0.008986  0.006288  0.000082  0.005434   \n",
       "7  0.017264  0.018840  0.001952  0.013285  0.003753  0.000030  0.005569   \n",
       "8  0.032650  0.016863  0.003349  0.014710  0.005086  0.000054  0.004866   \n",
       "\n",
       "   Unnamed: 70  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          NaN  \n",
       "7          NaN  \n",
       "8          NaN  \n",
       "\n",
       "[9 rows x 71 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_info_df = pd.read_csv(\"data/processed/output_POS.txt\", \n",
    "                           delimiter='|',\n",
    "                           index_col=False,\n",
    "                           quoting=3, \n",
    "                           encoding='utf-8')\n",
    "\n",
    "print(book_info_df.shape)\n",
    "book_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
